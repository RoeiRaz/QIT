\documentclass[a4paper,10pt]{hw}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

% Bra-Ket hack from stack exchange
\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1 \delimsize\vert #2}

% TabularX streched-centered column (Z)
\newcolumntype{Z}{>{\vfill\centering\let\newline\\\arraybackslash\hspace{0pt}}X}

%opening
\title{Introduction to Quantum Information Processing}
\author{Roei Rosenzweig 313590937,\\ Roey Maor 205798440}

\begin{document}

\maketitle

\section{Mutual Information - Solution}

\begin{enumerate}

\item % Question 1, sub section 1

We expand the expression to get:
$$I(X;Y) = H(X) - H(X|Y) = -\sum_{x}p(x)\log_{2}p(x) + \sum_{x,y}p(x,y)\log_{2}p(x|y)$$
We know that a probability distribution of variable $X$ can be computed from the mutual probability of $X$ and $Y$ by summing all the values in range for $Y$:
$$p(x) = \sum_{y}p(x,y)$$
Plugging this into the previous equation, we get:
$$I(X;Y) = \sum_{x,y}p(x,y)\log_{2}p(x|y) - \sum_{x,y}p(x,y)\log_{2}p(x) = \sum_{x,y}p(x,y)\log_{2}\left(\frac{p(x|y)}{p(x)}\right)$$
From logarithm rules and conditional probability definition:
$$\log_{2}\left(\frac{p(x|y)}{p(x)}\right) = -\log_{2}\left(\frac{p(x)p(y)}{p(x,y)}\right)$$
Plugging it, we get:
$$\boxed{ I(X;Y) = -\sum_{x,y}p(x,y)\log_{2}\left(\frac{p(x)p(y)}{p(x,y)}\right)}$$

\item % Question 1, sub section 2

We can write $I(X;Y)$ using the logarithm-identity $\log_{a}x \cdot \log_{b}a = \log_{b}x$ as:

$$ K\cdot I(X;Y) = -\sum_{x,y}p(x,y)\ln\left(\frac{p(x)p(y)}{p(x,y)}\right) $$

Where $K = \ln 2 > 0$. By using the identity $\ln t \leq t - 1$ for $t > 0$ and knowing that $\frac{p(x)p(y)}{p(x,y)}$ is non-negative (because probability cannot be negative, obv.) we show:

$$ - K\cdot I(X;Y) \leq \sum_{x,y}p(x,y)\left(\frac{p(x)p(y)}{p(x,y)} - 1\right)
	= \sum_{x,y}\left(p(x)p(y) - p(x,y)\right) $$

$\ln t \leq t - 1$ is equality \textbf{iff} $t = 1$.
\begin{corollary}
\label{corollary:1}
$-  K\cdot I(X;Y) = \sum_{x,y}\left(p(x)p(y) - p(x,y)\right) $ \emph{iff} $X, Y$ are independent. \textbf{Proof:} If $X, Y$ are independent $\iff$ $p(x)p(y) = p(x,y)$ for every pair of $x,y$ $\iff$ $\forall x,y: \frac{p(x)p(y)}{p(x,y)} = 1$ $\iff$
\end{corollary}

We expand the expression to get sums over probabilities (which we can reduce to $1$):

$$ - K\cdot I(X;Y) \leq \sum_{x} \left(p(x) \cdot \sum_{y} p(y)\right) - \sum_{x,y}p(x,y) $$

$$ \Rightarrow I(X;Y) \geq K \cdot \sum_{x} p(x) - 1 = 0 \Rightarrow \boxed{I(X;Y) \geq 0}$$

From \ref{corollary:1} and the above expansion we can conclude that $I(X;Y) = 0$ $\iff$ $X, Y$ are independent.


\end{enumerate}


\section{Entropy and Mutual Information}

\begin{enumerate}

\item % Question 2, sub section 1

$$
\begin{array}{rcl}
Y &=& \begin{cases} 1,& \text{the keys are in the pocket} \\ 0, & otherwise \end{cases} \\
X &=& \begin{cases} i\in[1,100],&\text{the keys are in the i-th location} \\ 0, & \text{otherwise} \end{cases}
\end{array}
$$

\item % Question 2, sub section 2

%TODO this looks bad

\hfill\\

\newcommand{\pyzxz}{
$$
\begin{array}{lcl}
p(X=0, Y=0) & = & 0\\
p(X=0 | Y=0) & = & 0\\
p(Y=0 | X=0) & = & 0
\end{array}
$$
}

\newcommand{\pylxz}{
$$
\begin{array}{lcl}
p(X=0, Y=1) & = & 0.99\\
p(X=0 | Y=1) & = & 1\\
p(Y=1 | X=0) & = & 1
\end{array}
$$
}

\newcommand{\pylxi}{
$$
\begin{array}{lcl}
i > 0 &&\\
p(X=i, Y=1) & = & 0\\
p(X=i | Y=1) & = & 0\\
p(Y=1 | X=i) & = & 0
\end{array}
$$
}

\newcommand{\pyzxi}{
$$
\begin{array}{lcl}
i > 0 &&\\
p(X=i, Y=0) & = & 0.0001\\
p(X=i | Y=0) & = & 0.01\\
p(Y=0 | X=i) & = & 1
\end{array}
$$
}

\begin{center}
%\rowcolors{1}{cyan}{white}
\begin{tabularx}{\textwidth}{c |ZZZ}
 				& 		$Y = 0$ 		& 		$Y = 1$		&		 					\tabularnewline
\hline
$X = 0$ 		&  		\pyzxz			& 		\pylxz		& $p(X = 0) = 0.99$			\tabularnewline
$X = i > 0$ 	& 		\pylxi			& 		\pyzxi		& $p(X = i > 0) = 0.0001$ 	\tabularnewline
			 	& $p(Y = 0) = 0.01$			& $p(Y = 1) = 0.99$		& $1$ 				\tabularnewline
\end{tabularx}
\end{center}

\item

\begin{flalign*}
H(X) 	&= -\sum_{x}p(X = x)\log_{2}p(X = x)&\\
 		&= -p(X = 0)\log_{2}p(X = 0) -100\cdotp(X = i)\log_{2}p(X = i) \\
 		&= -0.99\cdot\log_{2}0.99 - 100\cdot0.0001\log_{2}0.0001\\
 		&= 0.044
\end{flalign*}


\begin{flalign*}
H(Y) 	&= -\sum_{y}p(Y = y)\log_{2}p(Y = y) &\\
 		&= -p(Y = 0)\log_{2}p(Y = 0) -\cdotp(Y = 1)\log_{2}p(Y = 1) \\
 		&= -0.01\cdot\log_{2}0.01 - 0.99\log_{2}0.99\\
 		&= 0.024
\end{flalign*}


\begin{flalign*}
H(X|Y=0) 	&= -\sum_{x}p(X|Y = 0)\log_{2}p(X|Y = 0)&\\
 		&= -p(X = 0|Y = 0)\log_{2}p(X = 0|Y = 0) -100\cdotp(X = i|Y = 0)\log_{2}p(X = i|Y = 0) \\
 		&= -100\cdot0.01\log_{2}0.01\\
 		&= 2
\end{flalign*}

\begin{flalign*}
H(X|Y=1) 	&= -\sum_{x}p(X|Y = 1)\log_{2}p(X|Y = 1)&\\
 		&= -p(X = 0|Y = 1)\log_{2}p(X = 0|Y = 1) -100\cdotp(X = i|Y = 1)\log_{2}p(X = i|Y = 1) \\
 		&= -1\cdot 0\\
 		&= 0
\end{flalign*}

\begin{flalign*}
H(X|Y) 	&= -\sum_{x,y}p(X=x, Y=y)\log_{2}p(X = x|Y = y)&\\
 		&= -p(X = 0, Y = 0)\log_{2}p(X = 0|Y = 0) -\sum_{i=1}^{100}p(X = i, Y = 0)\log_{2}p(X = i|Y = 0)\\
 		&\hspace{10pt} -p(X = 0, Y = 1)\log_{2}p(X = 0|Y = 1) -\sum_{i=1}^{100}p(X = i, Y = 1)\log_{2}p(X = i|Y = 1) \\ 
 		&= 0 -100\cdot 0.0001\log_{2}0.01 -0.99\log_{2}1 -100\cdot 0 \\
 		&= 0.02
\end{flalign*}

\begin{flalign*}
H(Y|X) 	&= -\sum_{x,y}p(X=x, Y=y)\log_{2}p(Y = y|X = x)&\\
 		&= -p(X = 0, Y = 0)\log_{2}p(Y = 0|X = 0) -\sum_{i=1}^{100}p(X = i, Y = 0)\log_{2}p(Y = 0|X = i)\\
 		&\hspace{10pt} -p(X = 0, Y = 1)\log_{2}p(Y = 1|X = 0) -\sum_{i=1}^{100}p(Y = 1, X = i)\log_{2}p(Y = 1|X = i) \\ 
 		&= 0 -0 -0 -0 \\
 		&= 0
\end{flalign*}

% Question 2, sub section 4
\item
\begin{flalign*}
I(X;Y) &= H(X) - H(X|Y) \\
&= 0.044 - 0.02 = 0.024
\end{flalign*}


\begin{flalign*}
I(Y;X) &= H(Y) - H(Y|X) \\
&= 0.024 - 0 = 0.024
\end{flalign*}

% Question 2, sub section 5
\item
When we learn that the keys are not in the pocket, the entropy raises: $H(X|Y=0) > H(X)$. It seems strange because by "learning" something about the system, we reduced our information, but it is only guaranteed that \textbf{on average} the entropy is reduced when learning. The amount of entropy $H(X)$ reduced on average when learning the value of random variable $Y$ is $I(X;Y)$, and this value is always non-negative.

\end{enumerate}

\section{Poincare Sphere}

\begin{enumerate}

% Question 3, sub section 1
\item

\begin{itemize}

	\item \textbf{Direction: $\braket{\phi}{\phi^\prime} = 0 \Rightarrow$ $\ket{\phi}$ and $\ket{\phi^\prime}$ are on opposite sides of poincar\'{e} sphere } \\
	
	If $\braket{\phi}{\phi^\prime} = 0$ then:
	
	$$
	\begin{pmatrix}
	\cos{\frac{\theta}{2}} && e^{-i\phi}\sin{\frac{\theta}{2}}
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
	\cos{\frac{\theta^\prime}{2}} \\ e^{i\phi^\prime}\sin{\frac{\theta^\prime}{2}}
	\end{pmatrix}
	=
	0
	$$

	Unpacking this equation, we get:
	
	$$
	(\star) = \cos{\frac{\theta}{2}} \cos{\frac{\theta^\prime}{2}} + e^{i(\phi^\prime - \phi)}\sin{\frac{\theta}{2}} \sin{\frac{\theta^\prime}{2}} = 0
	$$
	
	In particular, $Im(\star) = 0$, thus $\phi \equiv \phi^\prime(\mod\pi)$. We are left with:
	
	$$
	\cos{\frac{\theta}{2}} \cos{\frac{\theta^\prime}{2}} 
	\pm \sin{\frac{\theta}{2}} \sin{\frac{\theta^\prime}{2}} 
	= 0 
	\Rightarrow 
	\sin\left(\frac{\pi}{2} 
	+ \frac{\theta}{2}\right)cos{\frac{\theta^\prime}{2}} 
	= \pm \cos\left(\frac{\pi}{2} + \frac{\theta}{2}\right)\sin{\frac{\theta^\prime}{2}}
	$$
	
	Which we can simplify to
	
	$$
	\tan\left(\frac{\pi}{2} + \frac{\theta}{2}\right) = \mp\tan{\frac{\theta^\prime}{2}}
	\Rightarrow
	\pi + \theta = \mp\theta^\prime
	$$
	
	Thus we get:
	
	$$
	\theta^\prime = \begin{cases}
	\theta + \pi, & \phi^\prime = \phi + \pi \\
	-(\theta + \pi), & \phi^\prime = \phi \end{cases}
	$$
	
	Obviously, the second case is impossible because then $\theta^\prime$ is negative. So we must conclude that:
	
	\begin{align*}
	\theta^\prime & =  \theta + \pi \\
	\phi^\prime & =  \phi + \pi
	\end{align*}
	
	\item \textbf{Direction: $\ket{\phi}$ and $\ket{\phi^\prime}$ on opposite direction $\Rightarrow$ $\braket{\phi}{\phi^\prime} = 0$} \\
	Suppose $\phi = \pi + \phi^\prime$ and $\theta = \pi + \theta^\prime$. Recall $(\star)$ from the previous direction of the proof, and plug in:
	\begin{align*}
	(\star) & = \cos{\frac{\theta}{2}} \cos{\frac{\theta + \pi}{2}} + e^{i(\pi + \phi - \phi)}\sin{\frac{\theta}{2}} \sin{\frac{\theta + \pi}{2}} \\
	& = \cos{\frac{\theta}{2}} \cos{\frac{\theta + \pi}{2}} - \sin{\frac{\theta}{2}} \sin{\frac{\theta + \pi}{2}} \\
	& = \cos{\frac{\theta}{2}} \sin{\frac{\theta}{2}} - \sin{\frac{\theta}{2}} \cos{\frac{\theta}{2}} = 0
	\end{align*}
\end{itemize}

% Question 3 sub section 2
\item
There is an infinite amount of ensembles, and exactly one ensembles that consists of 2 orthogonal states.

% Question 3 sub section 3
\item
There is an infinite amount of ensembles, all of which are pairs of orthogonal states.

\end{enumerate}

\section{Measurements}

\begin{enumerate}

% Question 4 sub section 1
\item

\begin{itemize}

	\item
	$ Pr(\ket{+}) = |\braket{+}{\phi}|^2 = \frac{1}{2} \cdot (\alpha + \beta)^2
	 $

	\item
	$ Pr(\ket{\theta}) = |\braket{\theta}{\phi}|^2 = \alpha^2\cos^2\theta - 2\alpha\beta\cos\theta\sin\theta + \beta^2\sin^2\theta = 1 -\alpha\beta\sin{2\theta}
	$

\end{itemize}

% Question 4 sub section 2
\item

\begin{itemize}

	\item
	$Pr(\ket{10}) = \bra{10}p\ket{10} = p_2$
	
	\item
	$Pr(\ket{00}) = \bra{00}p\ket{00} = p_0$

\end{itemize}

% Question 4 sub section 3

\begin{itemize}

	\item
	$$\ket{-+} = 
	\left(\frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ -1 \end{pmatrix}\right) 
	\otimes \left(\frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \end{pmatrix}\right)
	=
	\frac{1}{2} \begin{pmatrix} 1 \\ 1 \\ -1 \\ -1 \end{pmatrix}
	$$
	Thus,
	$$
	Pr({\ket{-+}}) = \frac{1}{4} \begin{pmatrix} 1 & 1 & -1 & -1 \end{pmatrix}
	\begin{pmatrix}
	p0 & 0 & 0 & 0 \\
	0 & p1 & 0 & 0 \\
	0 & 0 & p2 & 0 \\
	0 & 0 & 0 & p3
	\end{pmatrix}
	\begin{pmatrix} 1 \\ 1 \\ -1 \\ -1 \end{pmatrix}
	=
	\frac{p0 + p1 + p2 + p3}{4}
	$$	
	
	\item
	$$\ket{1+} = 
	\begin{pmatrix} 0 \\ 1 \end{pmatrix}
	\otimes \left(\frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \end{pmatrix}\right)
	=
	\frac{1}{\sqrt{2}} \begin{pmatrix} 0 \\ 0 \\ 1 \\ 1 \end{pmatrix}
	$$
	Thus,	
	$$
	Pr({\ket{1+}}) = \frac{1}{2} \begin{pmatrix} 0 & 0 & 1 & 1 \end{pmatrix}
	\begin{pmatrix}
	p0 & 0 & 0 & 0 \\
	0 & p1 & 0 & 0 \\
	0 & 0 & p2 & 0 \\
	0 & 0 & 0 & p3
	\end{pmatrix}
	\begin{pmatrix} 0 \\ 0 \\ 1 \\ 1 \end{pmatrix}
	=
	\frac{p2 + p3}{2}
	$$	
	
\end{itemize}

\end{enumerate}

\section{Double slit experiment}

\begin{enumerate}

% Question 5 sub section 1
\item
\begin{align*}
\vec{E_1}(\vec{r_1}, t) + \vec{E_2}(\vec{r_2}, t)
&= \frac{\vec{E_0}}{r_1}\cos{(k\cdot r_1 - w\cdot r)} + \frac{\vec{E_0}}{r_2}\cos{(k\cdot r_2 -w\cdot r)}\\
&\approx \frac{\vec{E_0}}{r} \left( \cos{(k\cdot r_1 - w\cdot r)}+\cos{(k\cdot r_2 -w\cdot r)}\right) \\
&= \boxed{2\frac{\vec{E_0}}{r}\cos{\left( \frac{k\cdot r_1 + k\cdot r_2 - 2w\cdot t}{2} \right)} \cos{\left(\frac{k(r_1 - r_2)}{2}\right)}}
\end{align*}

% Question 5 sub section 2
\item
\begin{align*}
|\vec{E_1}(\vec{r_1}, t) + \vec{E_2}(\vec{r_2}, t)|^2
&= \left|2\frac{\vec{E_0}}{r}\cos{\left( \frac{k\cdot r_1 + k\cdot r_2 - 2w\cdot t}{2} \right)} \cos{\left(\frac{k(r_1 - r_2)}{2}\right)}\right|^2 \\
&= \frac{4E_0^2}{r^2}\left|\cos{\left(\frac{k(r_1 - r_2)}{2}\right)}\right|^2 \left|\cos{\left( \frac{k\cdot r_1 + k\cdot r_2 - 2w\cdot t}{2} \right)}\right|^2
\end{align*}

% Question 5 sub section 3
\item
\begin{align*}
\left<|\vec{E_1}(\vec{r_1}, t) + \vec{E_2}(\vec{r_2}, t)|^2\right>
&= \left<\frac{4E_0^2}{r^2}\left|\cos{\left(\frac{k(r_1 - r_2)}{2}\right)}\right|^2 \left|\cos{\left( \frac{k\cdot r_1 + k\cdot r_2 - 2w\cdot t}{2} \right)}\right|^2\right>\\
&= \frac{4E_0^2}{r^2}\left|\cos{\left(\frac{k(r_1 - r_2)}{2}\right)}\right|^2 \left<\left|\cos{\left( \frac{k\cdot r_1 + k\cdot r_2 - 2w\cdot t}{2} \right)}\right|^2\right>\\
&= \frac{4E_0^2}{r^2}\left|\cos{\left(\frac{k(r_1 - r_2)}{2}\right)}\right|^2 \frac{1}{2}
\end{align*}

% Question 5 sub section 4
\item
$$
\boxed{I(\theta) = \frac{4E_0^2}{r^2}\left|\cos{\left(\frac{k\cos{\theta}}{2}\right)}\right|^2 \frac{1}{2}}
$$

\end{enumerate}

\end{document}




































