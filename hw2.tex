\documentclass[a4paper,10pt]{hw}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

% Bra-Ket hack from stack exchange
\DeclarePairedDelimiter\bra{\langle}{\rvert}
\DeclarePairedDelimiter\ket{\lvert}{\rangle}
\DeclarePairedDelimiterX\braket[2]{\langle}{\rangle}{#1 \delimsize\vert #2}

% TabularX streched-centered column (Z)
\newcolumntype{Z}{>{\vfill\centering\let\newline\\\arraybackslash\hspace{0pt}}X}

%opening
\title{Introduction to Quantum Information Processing}
\author{Roei Rosenzweig 313590937,\\ Roey Maor 205798440}

\begin{document}

\maketitle

\section{Mutual Information - Solution}

\begin{enumerate}

\item % Question 1, sub section 1

We expand the expression to get:
$$I(X;Y) = H(X) - H(X|Y) = -\sum_{x}p(x)\log_{2}p(x) + \sum_{x,y}p(x,y)\log_{2}p(x|y)$$
We know that a probability distribution of variable $X$ can be computed from the mutual probability of $X$ and $Y$ by summing all the values in range for $Y$:
$$p(x) = \sum_{y}p(x,y)$$
Plugging this into the previous equation, we get:
$$I(X;Y) = \sum_{x,y}p(x,y)\log_{2}p(x|y) - \sum_{x,y}p(x,y)\log_{2}p(x) = \sum_{x,y}p(x,y)\log_{2}\left(\frac{p(x|y)}{p(x)}\right)$$
From logarithm rules and conditional probability definition:
$$\log_{2}\left(\frac{p(x|y)}{p(x)}\right) = -\log_{2}\left(\frac{p(x)p(y)}{p(x,y)}\right)$$
Plugging it, we get:
$$\boxed{ I(X;Y) = -\sum_{x,y}p(x,y)\log_{2}\left(\frac{p(x)p(y)}{p(x,y)}\right)}$$

\item % Question 1, sub section 2

We can write $I(X;Y)$ using the logarithm-identity $\log_{a}x \cdot \log_{b}a = \log_{b}x$ as:

$$ K\cdot I(X;Y) = -\sum_{x,y}p(x,y)\ln\left(\frac{p(x)p(y)}{p(x,y)}\right) $$

Where $K = \ln 2 > 0$. By using the identity $\ln t \leq t - 1$ for $t > 0$ and knowing that $\frac{p(x)p(y)}{p(x,y)}$ is non-negative (because probability cannot be negative, obv.) we show:

$$ - K\cdot I(X;Y) \leq \sum_{x,y}p(x,y)\left(\frac{p(x)p(y)}{p(x,y)} - 1\right)
	= \sum_{x,y}\left(p(x)p(y) - p(x,y)\right) $$

$\ln t \leq t - 1$ is equality \textbf{iff} $t = 1$.
\begin{corollary}
\label{corollary:1}
$-  K\cdot I(X;Y) = \sum_{x,y}\left(p(x)p(y) - p(x,y)\right) $ \emph{iff} $X, Y$ are independent. \textbf{Proof:} If $X, Y$ are independent $\iff$ $p(x)p(y) = p(x,y)$ for every pair of $x,y$ $\iff$ $\forall x,y: \frac{p(x)p(y)}{p(x,y)} = 1$ $\iff$
\end{corollary}

We expand the expression to get sums over probabilities (which we can reduce to $1$):

$$ - K\cdot I(X;Y) \leq \sum_{x} \left(p(x) \cdot \sum_{y} p(y)\right) - \sum_{x,y}p(x,y) $$

$$ \Rightarrow I(X;Y) \geq K \cdot \sum_{x} p(x) - 1 = 0 \Rightarrow \boxed{I(X;Y) \geq 0}$$

From \ref{corollary:1} and the above expansion we can conclude that $I(X;Y) = 0$ $\iff$ $X, Y$ are independent.


\end{enumerate}


\section{Entropy and Mutual Information}

\begin{enumerate}

\item % Question 2, sub section 1

$$
\begin{array}{rcl}
Y &=& \begin{cases} 1,& \text{the keys are in the pocket} \\ 0, & otherwise \end{cases} \\
X &=& \begin{cases} i\in[1,100],&\text{the keys are in the i-th location} \\ 0, & \text{otherwise} \end{cases}
\end{array}
$$

\item % Question 2, sub section 2

%TODO this looks like crap

\hfill\\

\newcommand{\pyzxz}{
$$
\begin{array}{lcl}
p(X=0, Y=0) & = & 0\\
p(X=0 | Y=0) & = & 0\\
p(Y=0 | X=0) & = & 0
\end{array}
$$
}

\newcommand{\pylxz}{
$$
\begin{array}{lcl}
p(X=0, Y=1) & = & 0.99\\
p(X=0 | Y=1) & = & 1\\
p(Y=1 | X=0) & = & 1
\end{array}
$$
}

\newcommand{\pylxi}{
$$
\begin{array}{lcl}
i > 0 &&\\
p(X=i, Y=1) & = & 0\\
p(X=i | Y=1) & = & 0\\
p(Y=1 | X=i) & = & 0
\end{array}
$$
}

\newcommand{\pyzxi}{
$$
\begin{array}{lcl}
i > 0 &&\\
p(X=i, Y=0) & = & 0.0001\\
p(X=i | Y=0) & = & 0.01\\
p(Y=0 | X=i) & = & 1
\end{array}
$$
}

\begin{center}
%\rowcolors{1}{cyan}{white}
\begin{tabularx}{\textwidth}{c |ZZZ}
 				& 		$Y = 0$ 		& 		$Y = 1$		&		 					\tabularnewline
\hline
$X = 0$ 		&  		\pyzxz			& 		\pylxz		& $p(X = 0) = 0.99$			\tabularnewline
$X = i > 0$ 	& 		\pylxi			& 		\pyzxi		& $p(X = i > 0) = 0.0001$ 	\tabularnewline
			 	& $p(Y = 0) = 0.01$			& $p(Y = 1) = 0.99$		& $1$ 				\tabularnewline
\end{tabularx}
\end{center}

\item

\begin{flalign*}
H(X) 	&= -\sum_{x}p(X = x)\log_{2}p(X = x)&\\
 		&= -p(X = 0)\log_{2}p(X = 0) -100\cdotp(X = i)\log_{2}p(X = i) \\
 		&= -0.99\cdot\log_{2}0.99 - 100\cdot0.0001\log_{2}0.0001\\
 		&= 0.044
\end{flalign*}


\begin{flalign*}
H(Y) 	&= -\sum_{y}p(Y = y)\log_{2}p(Y = y) &\\
 		&= -p(Y = 0)\log_{2}p(Y = 0) -\cdotp(Y = 1)\log_{2}p(Y = 1) \\
 		&= -0.01\cdot\log_{2}0.01 - 0.99\log_{2}0.99\\
 		&= 0.024
\end{flalign*}


\begin{flalign*}
H(X|Y=0) 	&= -\sum_{x}p(X|Y = 0)\log_{2}p(X|Y = 0)&\\
 		&= -p(X = 0|Y = 0)\log_{2}p(X = 0|Y = 0) -100\cdotp(X = i|Y = 0)\log_{2}p(X = i|Y = 0) \\
 		&= -100\cdot0.01\log_{2}0.01\\
 		&= 2
\end{flalign*}

\begin{flalign*}
H(X|Y=1) 	&= -\sum_{x}p(X|Y = 1)\log_{2}p(X|Y = 1)&\\
 		&= -p(X = 0|Y = 1)\log_{2}p(X = 0|Y = 1) -100\cdotp(X = i|Y = 1)\log_{2}p(X = i|Y = 1) \\
 		&= -1\cdot 0\\
 		&= 0
\end{flalign*}

\begin{flalign*}
H(X|Y) 	&= -\sum_{x,y}p(X=x, Y=y)\log_{2}p(X = x|Y = y)&\\
 		&= -p(X = 0, Y = 0)\log_{2}p(X = 0|Y = 0) -\sum_{i=1}^{100}p(X = i, Y = 0)\log_{2}p(X = i|Y = 0)\\
 		&\hspace{10pt} -p(X = 0, Y = 1)\log_{2}p(X = 0|Y = 1) -\sum_{i=1}^{100}p(X = i, Y = 1)\log_{2}p(X = i|Y = 1) \\ 
 		&= 0 -100\cdot 0.0001\log_{2}0.01 -0.99\log_{2}1 -100\cdot 0 \\
 		&= 0.02
\end{flalign*}

\begin{flalign*}
H(Y|X) 	&= -\sum_{x,y}p(X=x, Y=y)\log_{2}p(Y = y|X = x)&\\
 		&= -p(X = 0, Y = 0)\log_{2}p(Y = 0|X = 0) -\sum_{i=1}^{100}p(X = i, Y = 0)\log_{2}p(Y = 0|X = i)\\
 		&\hspace{10pt} -p(X = 0, Y = 1)\log_{2}p(Y = 1|X = 0) -\sum_{i=1}^{100}p(Y = 1, X = i)\log_{2}p(Y = 1|X = i) \\ 
 		&= 0 -0 -0 -0 \\
 		&= 0
\end{flalign*}

% Question 2, sub section 4
\item

\end{enumerate}

\section{Poincare Sphere}

\begin{enumerate}

% Question 3, sub section 1
\item

\begin{itemize}

	\item \textbf{Direction: $\braket{\phi}{\phi^\prime} = 0 \Rightarrow$ $\ket{\phi}$ and $\ket{\phi^\prime}$ are on opposite sides of poincar\'{e} sphere } \\
	
	If $\braket{\phi}{\phi^\prime} = 0$ then:
	
	$$
	\begin{pmatrix}
	\cos{\frac{\theta}{2}} && e^{-i\phi}\sin{\frac{\theta}{2}}
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
	\cos{\frac{\theta^\prime}{2}} \\ e^{i\phi^\prime}\sin{\frac{\theta^\prime}{2}}
	\end{pmatrix}
	=
	0
	$$

	Unpacking this equation, we get:
	
	$$
	(\star) = \cos{\frac{\theta}{2}} \cos{\frac{\theta^\prime}{2}} + e^{i(\phi^\prime - \phi)}\sin{\frac{\theta}{2}} \sin{\frac{\theta^\prime}{2}} = 0
	$$
	
	In particular, $Im(\star) = 0$, thus $\phi \equiv \phi^\prime(\mod\pi)$ (The states are in opposite directions w.r.t. $\phi$ and $\phi^\prime$). We are left with:
	
	$$
	\cos{\frac{\theta}{2}} \cos{\frac{\theta^\prime}{2}} + \sin{\frac{\theta}{2}} \sin{\frac{\theta^\prime}{2}} = 0 \Rightarrow \sin\left(\frac{\pi}{2} + \frac{\theta}{2}\right)\cos{\frac{\theta^\prime}{2}} = \cos\left(\frac{\pi}{2} + \frac{\theta}{2}\right)\sin{\frac{\theta^\prime}{2}}
	$$
	
	Which we can simplify to
	
	$$
	\tan\left(\frac{\pi}{2} + \frac{\theta}{2}\right) = \tan{\frac{\theta^\prime}{2}}
	\Rightarrow
	\pi + \theta = \theta^\prime
	$$
	
	\item \textbf{Direction: $\ket{\phi}$ and $\ket{\phi^\prime}$ on opposite direction $\Rightarrow$ $\braket{\phi}{\phi^\prime} = 0$} \\
	Suppose $\phi = \pi + \phi^\prime$ and $\theta = \pi + \theta^\prime$. Recall $(\star)$ from the previous direction of the proof, and plug in:
	$$
	\cos{\frac{\theta}{2}} \cos{\frac{\theta + \pi}{2}} + e^{i(\phi^\prime - \phi)}\sin{\frac{\theta}{2}} \sin{\frac{\theta^\prime}{2}}
	$$
\end{itemize}

\end{enumerate}

\end{document}




































